{% extends "master.html" %}

{% block title %} Policy Vacuums {% endblock %}

{% block head %}

<link rel="stylesheet" href="../css/desc.css">

{% endblock %}

{% block content %}

<div class="information">

    <h1 class="informationHeader">Policy Vacuums</h1>

    <div class="informationParagraphs">
        <p>
            Open letter from some of the world’s top scientists that say that piloting drones and the stuff like that is ok because
            it’s still the human’s decision, but autonomous machines that kill should be banned.
        </p>
        <p>
            “Autonomous and semi-autonomous weapon systems shall be designed to allow commanders and operators to exercise appropriate
            levels of human judgement over the use of force.”
            “Human supervised autonomous weapons systems may be used to select and engage targets, with the exception of human targets,
            for local defense to intercept time critical or saturation attempts.”
        </p>
        <p>
            The above two quotes were ones that I found to be very significant when it came to the policies that are associated with AI.
            I pulled them straight from a document released by the US Department of Defense around 2012. The entire document deals with laws and
            policies which will govern how the military should handle these autonomous robots, but misses a lot of key points about what we should do
            in what situations. Like who does get blamed if a robot does kill a human being? What if the commander is unable to stop the robot? What then?
            There’s a section on what portion is responsible for the upkeep and hiring of personnel but nothing on training soldiers on the machines.
        </p>
        <p>
            open letter to the military sent by the top AI researchers around the world (http://futureoflife.org/ai-open-letter/)
        </p>
        <p>
            The Three Laws of Robotics
            1.)	A robot may not injure a human being or, through inaction, allow a human being to
            come to harm.
            2.)	A robot must obey orders given it by human beings except where such orders go against the first law.
            3.)	A robot must protect its own existence as long as such protection does not conflict with
            the first and second laws.
        </p>

        <p>
            I read an interesting article that talked generally about the ideas of AI and how we need to beware of the dangers of it.
            Well an interesting portion of this article dealt with the three laws which I have above and talked about how they were much too simple
            for the problems we have with robotics nowadays. Because robots can be built for a multitude of things (including war), the laws may not
            apply to every robot, and involve a lot of contradictions involving “over-protection” and things like that.
        </p>


    </div>

</div>


{% endblock %}
